a# 제목 : 데이터분석 개념 및 기초

# 함수
    1. 일만하고 끝내는 함수
    2. 일한 결과를 다시 저장하는 함수 return (한개만 반환)

# 파이썬 모듈 종류
    1. 표준모듈
    2. 외부모듈
    3. 사용자모듈

    requests 모듈
        - 파이썬 사용자들을 위해 만들어진 간단한 파이썬용 HTTP 라이브러리이며, 간단하게는
          HTTP, HTTPS 웹 사이트에 요청하기 위해 자주 사용되는 모듈 중 하나

        - Crawling 과정에서 requests 모듈을 이용해 웹사이트의 소스코드를 가져온 다음
          파싱을 하는 경우가 많음

    
# HTTP 요청/응답 구조
    - 사용자가 서버에 요청을 할때는 크게 4개로 구분하여 요청할 수 있음

        GET     : 정보를 가져오기 위해 요청
        POST    : 새로운 정보를 보내기 위해 요청
        PUT     : 수정할 정보를 보내기 위해 요청
        DELETE  : 정보를 삭제하기 위해 요청  


# 파이썬의 문자열 및 함수를 이용한 데이터 가공
    - 문자열 [인덱스 : 인덱스2] : 슬라이싱(자르기)

    - 문자열 관련 함수 (문자열.함수)
        ● index('찾는 문자열'), find('찾는 문자열') : 문자열의 위치
        ● strip()   : 죄우 공백을 제거
        ● replace() : 문자열을 바꾸기
        ● 정규표현식 : re.sub('찾는 패턴', '대체할 문자열', 대상문자열)
        ● split()   : 단어를 분리해서 리스트 자료로 생성

    - 정규표현식을 이용하기 (import re)
        ● [] : 괄호안에 문자와 매치되면 찾음
        ● *  : 문자가 반복되는 것을 찾음 (0번 반복도 포함) {m,n} 일정한 반복
          i.    + : 1번부터 반복되는 것을 찾음
        ● .  : 모든 문자를 의미함(단 .* 은 어떤 문자들의 반복)  
        ● ^  : 반대라는 의미 ([^0-9] : 숫자가 아닌 것 찾음)
        ● -  : 범위를 의미함 ([a-z] : 소문자 모두)
        ● ￦ : 특수한 의미 (￦d == [0-9] 같다)
    
    - 파이썬에 함수 사용 방법 (한 개씩 검색)
        ● pattern = '패턴'
        ● m = re.search(pattetn, text) # 한 개만 찾음
        ● result = m.group() # 매칭된 글자를 가져옴

    - 파이썬에 함수사용 방법 (여러 개 검색)
        ● pattern = '패턴'
        ● datas = re.findall(pattern, )

    - 팡썬에서 함수(특정문자들) 제거 방법
        ● pattern = '패턴'
        ● re.sub(pattern, '공백', text)



# 파이썬 활용 데이터 분석

    - requests (웹통신 통해 데이터 가져옴), re (텍스트의 패턴을 검색) 활용한 데이터 가공
        ● http를 통해서 해당 URL 페이지에 접속
            ○ html_data = response.text
        ● 코드를 짜기 전에 대략적인 html, css 등을 분석해야 함 (제목, 목록들의 패턴)
        ● 파이썬 활용한 코드
            ○ 인덱싱, 슬라이싱 text[0:10]
            ○ 정규식을 통해서 원하는 태그를 찾거나 제거함

    - 


# 파이썬 문법을 사용해서 파일 처리하는 방법
    - 주로 사용하는 파일 종류 : txt, csv, json, xlsx
    - 파일로 저장된 데이터를 읽기 위한 함수
        ● Fr = open('파일 경로', 'r', encoding='문자셋')
        ● Line = Fr.readline() 한 줄씩 읽어옴

    - 영화 장르 데이터 (movies.csv) 파일을 읽어서 분석
        ● 파일 형식
            일련번호, 영화제목, 장르(복수)
        ● 데이터에 있는 장르 종류가 뭐임
            파일읽기(open 함수), csv 모듈 읽음(import csv)

# BeautifulSoup 모듈
    - 개요 : HTML 기반의 텍스트를 파싱하여 검색해주는 외부 모듈

    - 사용법
        ● 설치 : pip install BeautifulSoup4
        ● 호출 : from bs4 import BeautifulSoup
        ● 생성 : soup = BeautifulSoup(html_doc, 'html.parser')
        ● 확인 : print(soup.prettify())

    - 모듈이므로 함수 사용법을 익힘
        ● Tag = find('태그명','속성=속성값')    # 첫번째만 찾음
        ● Tag = find_all('태그명','속성=속성값')    # 여러 개 찾음

    - CSS 선택자
        ● 태그 선택자
        ● 아이디 선택자
        ● 클래스 선택자
        ● 부모자식 선택자(직계) ex) 부모태그 > 자식태크
        ● 자손 선택자(직계 또는 아니어도 됨) 할아버지태그(공백) 손자태그
        ● 속성선택자, 형제선택자 : 특수한 경우

    - 함수 (select, select_one) 사용법을 익힘
        ● Select('div')
        ● Select('h1#logo')
        ● Select('ul.menu')
        ● Select('ul>li')
        ● Select('ul a') <ul><li><a hrdf='#>텍스트</a></li></ul>

    - 태그를 찾은 후 태그 안에 글자를 가져오려면 
        ● get_text() : 태그 안에 글자를 가져옴 (태그가 밖에 여러 개 있어도 찾음)
        ● 결과.string : 태그 안에 글자를 가져옴 (태그가 한 개만 있을 때)
        ● get(속성명) ex) atag.get('href')


# Selenium 모듈(외부)
    - 개요
        ● 웹사이트를 테스트하거나 자동화하는 도구가 목적
        ● BeautifulSoup과 같이 활용해서 크롤링에 사용 목적 

    - 설치
        ● pip install selenium # 라이브러리 설치
            버전 : 4.10
        ● webdriver 다운로드 # 브라우저별로 버전이 맞는 파일
        ● 브라우저 호출방법
            ○ WebdriverChrome(브라우저 경로나 매니저) # 4.8 이전 버전
            ○ WebdriverChrome()                     # 4.10 이후 버전

    - 라이브러리 함수들
        ● 기본적으로 자바스크립트의 방식과 유사(브라우저 호출하고 해당 요소를 찾음)
        ● Driver = webdriver.Chrome()
        ● Driver.find_element(By.ID, '아이디명')
        ● Driver.find_elements(By.CLASS_NAME, '클래스명')
        ● Driver.find_elements(By.CSS_SELECTOR, '선택자')
        ● Driver.find_elements(By.TAG_NAME, '태그명')
        ● 값을 찾는 경우 (.tag_name, .text)
        ● 원래 url 에서 다른 url로 변경되는 경우가 있음
          new_url = driver.current_url
        ● 이벤트 관련 함수 ㅣ html 요소를 선택하고 액션을 제어할 수 있음
          click(), send_keys(),
          Keys.ENTER(엔터, 백스페이스, 홈 등등)
        ● 제어를 모두 완료하게 되면 해당 페이지의 소스를 가져올 수 있음
          html_doc = driver.page_source
             
    
